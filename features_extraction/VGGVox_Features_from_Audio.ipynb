{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import pickle, gzip\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from numpy.fft import fft\n",
    "import math\n",
    "from scipy.signal import lfilter, stft\n",
    "from scipy.stats import zscore\n",
    "from scipy.io import wavfile\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a symbolic link to the database\n",
    "\n",
    "The utterance .wav files should be located in folder `audio` and features written to folder `features`. Modify as needed. The .wav folder structure is:\n",
    "\n",
    "```\n",
    "audio/  \n",
    "   +---CHN/\n",
    "   |     +----G0021/\n",
    "   |             |------G00021S1053.wav\n",
    "   |             |------   .\n",
    "   |             |------   .\n",
    "   +---IND\n",
    "   +--- .\n",
    "   +--- .\n",
    "   +---US\n",
    "```   \n",
    "   \n",
    "If the database is located in harddisk, create a symbolic link to this notebook's working directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ln: /Users/samsudinng/MSAI/Accent/AccentRecognition/audio/audio: File exists\n",
      "ln: /Users/samsudinng/MSAI/Accent/AccentRecognition/features/features: File exists\n"
     ]
    }
   ],
   "source": [
    "#create symbolic link as database is located in harddisk\n",
    "!ln -s /Volumes/AIWorks/accented_ASR/audio/ ~/MSAI/Accent/AccentRecognition/audio\n",
    "!ln -s /Volumes/AIWorks/accented_ASR/features/ ~/MSAI/Accent/AccentRecognition/features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "# CONSTANTS AND LOOKUP TABLES \n",
    "#############################\n",
    "\n",
    "#to map accent from Kaldi utt2accent to the corresponding accent folder\n",
    "accent_dir = {\n",
    "    'BRITISH'   : 'UK',       \n",
    "    'AMERICAN'  : 'US',\n",
    "    'PORTUGUESE': 'PT',  \n",
    "    'KOREAN'    : 'KR',\n",
    "    'JAPANESE'  : 'JPN',\n",
    "    'RUSSIAN'   : 'RU',\n",
    "    'CHINESE'   : 'CHN',\n",
    "    'INDIAN'    : 'IND'   \n",
    "}\n",
    "\n",
    "#various spectrogram parameters\n",
    "spectrogram_params = {\n",
    "    'vggvox': {    # this is the same setting as VGG VoxCeleb1\n",
    "        'window'        : 'hamming',\n",
    "        'sampling_freq' : 16000,\n",
    "        'win_len'       : 25, #msec\n",
    "        'hop_len'       : 10, #msec\n",
    "        'ndft'          : 512,\n",
    "        'output_folder' : 'vggvox_logspec512'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Set the parameter here (file paths, spectrogram parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "# FILE RELATED \n",
    "############################\n",
    "\n",
    "# use same split as Kaldi features\n",
    "train_label_file ='./features_dump/trainv2/utt2accent'\n",
    "dev_label_file ='./features_dump/devv2/utt2accent'\n",
    "train_gender_label_file ='./features_dump/trainv2/utt2sex'\n",
    "dev_gender_label_file ='./features_dump/devv2/utt2sex'\n",
    "train_age_label_file ='./features_dump/trainv2/utt2age'\n",
    "dev_age_label_file ='./features_dump/devv2/utt2age'\n",
    "test_label_file ='./features_dump/testRmOOS/utt2accent'\n",
    "train_on_subset = {'activate': False, 'size':0.1}\n",
    "feature_setting = 'vggvox'\n",
    "\n",
    "\n",
    "############################\n",
    "# SPECTROGRAM PARAMETERS\n",
    "############################\n",
    "spec_params = spectrogram_params[feature_setting]\n",
    "\n",
    "#path to the audio\n",
    "wavdir = 'audio/'\n",
    "outdir = f\"features/{spec_params['output_folder']}/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get the wav file paths for each utterance in train, dev and test set\n",
    "\n",
    "The paths and metadata is contained in dataframes `train_df`, `dev_df` and `test_df`. Each dataframe contain the following columns:\n",
    "\n",
    "1. `utt`    : the utterance label\n",
    "2. `label`  : the accent label (int, {0 .. 7})\n",
    "3. `gender` : the gender label (int, {0 .. 1})\n",
    "4. `age`    : the age (int)\n",
    "5. `wavfile`: full path to the utterance's .wav file\n",
    "\n",
    "\n",
    "|    |utt\t                                        |label|gender    |age    |wavfile|\n",
    "|:---|:---|:---|:---|:---|:---|\n",
    "|0\t|AESRC2020-AMERICAN-ACCENT-G00473-G00473S1001\t|0\t  |1\t     |21\t |audio/US/G00473/G00473S1001.wav|\n",
    "|1\t|AESRC2020-AMERICAN-ACCENT-G00473-G00473S1002\t|0\t  |1\t     |21\t |audio/US/G00473/G00473S1002.wav|\n",
    "|2\t|AESRC2020-AMERICAN-ACCENT-G00473-G00473S1003\t|0\t  |1\t     |21\t |audio/US/G00473/G00473S1003.wav|\n",
    "|3\t|AESRC2020-AMERICAN-ACCENT-G00473-G00473S1004\t|0\t  |1\t     |21\t |audio/US/G00473/G00473S1004.wav|\n",
    "|4\t|AESRC2020-AMERICAN-ACCENT-G00473-G00473S1006\t|0\t  |1\t     |21\t |audio/US/G00473/G00473S1006.wav|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN set:\n",
      "# utterances: 124541\n",
      "                                            utt  label  gender  age  \\\n",
      "0  AESRC2020-AMERICAN-ACCENT-G00473-G00473S1001      0       1   21   \n",
      "1  AESRC2020-AMERICAN-ACCENT-G00473-G00473S1002      0       1   21   \n",
      "2  AESRC2020-AMERICAN-ACCENT-G00473-G00473S1003      0       1   21   \n",
      "3  AESRC2020-AMERICAN-ACCENT-G00473-G00473S1004      0       1   21   \n",
      "4  AESRC2020-AMERICAN-ACCENT-G00473-G00473S1006      0       1   21   \n",
      "\n",
      "                           wavfile  \n",
      "0  audio/US/G00473/G00473S1001.wav  \n",
      "1  audio/US/G00473/G00473S1002.wav  \n",
      "2  audio/US/G00473/G00473S1003.wav  \n",
      "3  audio/US/G00473/G00473S1004.wav  \n",
      "4  audio/US/G00473/G00473S1006.wav  \n",
      "\n",
      "DEV set:\n",
      "# utterances: 11988\n",
      "                                            utt  label  gender  age  \\\n",
      "0  AESRC2020-AMERICAN-ACCENT-G00007-G00007S1001      0       0   20   \n",
      "1  AESRC2020-AMERICAN-ACCENT-G00007-G00007S1002      0       0   20   \n",
      "2  AESRC2020-AMERICAN-ACCENT-G00007-G00007S1004      0       0   20   \n",
      "3  AESRC2020-AMERICAN-ACCENT-G00007-G00007S1007      0       0   20   \n",
      "4  AESRC2020-AMERICAN-ACCENT-G00007-G00007S1009      0       0   20   \n",
      "\n",
      "                           wavfile  \n",
      "0  audio/US/G00007/G00007S1001.wav  \n",
      "1  audio/US/G00007/G00007S1002.wav  \n",
      "2  audio/US/G00007/G00007S1004.wav  \n",
      "3  audio/US/G00007/G00007S1007.wav  \n",
      "4  audio/US/G00007/G00007S1009.wav  \n",
      "TEST set:\n",
      "# utterances: 14493\n",
      "                                            utt  label  \\\n",
      "0  AESRC2020-AMERICAN-ACCENT-G00006-G00006S1259      0   \n",
      "1  AESRC2020-AMERICAN-ACCENT-G00006-G00006S1260      0   \n",
      "2  AESRC2020-AMERICAN-ACCENT-G00006-G00006S2414      0   \n",
      "3  AESRC2020-AMERICAN-ACCENT-G00006-G00006S2415      0   \n",
      "4  AESRC2020-AMERICAN-ACCENT-G00006-G00006S3425      0   \n",
      "\n",
      "                           wavfile  \n",
      "0  audio/US/G00006/G00006S1259.wav  \n",
      "1  audio/US/G00006/G00006S1260.wav  \n",
      "2  audio/US/G00006/G00006S2414.wav  \n",
      "3  audio/US/G00006/G00006S2415.wav  \n",
      "4  audio/US/G00006/G00006S3425.wav  \n"
     ]
    }
   ],
   "source": [
    "# Train labels\n",
    "train_labels = pd.read_csv(train_label_file, names = ['utt','label'], header=None, \n",
    "                           delim_whitespace=True,index_col='utt')\n",
    "gender = pd.read_csv(train_gender_label_file, names = ['utt','gender'],header=None, \n",
    "                     delim_whitespace=True,index_col='utt')\n",
    "age = pd.read_csv(train_age_label_file, names = ['utt','age'], header=None, \n",
    "                  delim_whitespace=True,index_col='utt')\n",
    "train_labels['gender'] = gender\n",
    "train_labels['age'] = age\n",
    "train_labels.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "# Dev labels\n",
    "dev_labels = pd.read_csv(dev_label_file, names = ['utt','label'], header=None, \n",
    "                           delim_whitespace=True,index_col='utt')\n",
    "gender = pd.read_csv(dev_gender_label_file, names = ['utt','gender'],header=None, \n",
    "                     delim_whitespace=True,index_col='utt')\n",
    "age = pd.read_csv(dev_age_label_file, names = ['utt','age'], header=None, \n",
    "                  delim_whitespace=True,index_col='utt')\n",
    "dev_labels['gender'] = gender\n",
    "dev_labels['age'] = age\n",
    "dev_labels.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# Test labels\n",
    "test_labels = pd.read_csv(test_label_file, header=None, delim_whitespace=True) \n",
    "test_labels.columns=['utt','label']\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Train Set\n",
    "\"\"\"\n",
    "\n",
    "accent = train_labels.utt.str.split(pat='-',expand=True).iloc[:,1].map(accent_dir).tolist()\n",
    "speaker = train_labels.utt.str.split(pat='-',expand=True).iloc[:,3].tolist()\n",
    "utt  = train_labels.utt.str.split(pat='-',expand=True).iloc[:,4].tolist()\n",
    "\n",
    "wavfile = list(zip(accent,speaker,utt))\n",
    "train_labels['wavfile'] = [wavdir+'/'.join(s)+'.wav' for s in wavfile]\n",
    "train_df = train_labels\n",
    "print(f'TRAIN set:\\n# utterances: {train_labels.shape[0]}\\n{train_labels.head()}\\n')\n",
    "\n",
    "\"\"\"\n",
    "Dev Set\n",
    "\"\"\"\n",
    "\n",
    "accent = dev_labels.utt.str.split(pat='-',expand=True).iloc[:,1].map(accent_dir).tolist()\n",
    "speaker = dev_labels.utt.str.split(pat='-',expand=True).iloc[:,3].tolist()\n",
    "utt  = dev_labels.utt.str.split(pat='-',expand=True).iloc[:,4].tolist()\n",
    "\n",
    "wavfile = list(zip(accent,speaker,utt))\n",
    "dev_labels['wavfile'] = [wavdir+'/'.join(s)+'.wav' for s in wavfile]\n",
    "dev_df = dev_labels\n",
    "print(f'DEV set:\\n# utterances: {dev_labels.shape[0]}\\n{dev_labels.head()}')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Test Set\n",
    "\"\"\"\n",
    "\n",
    "accent = test_labels.utt.str.split(pat='-',expand=True).iloc[:,1].map(accent_dir).tolist()\n",
    "speaker = test_labels.utt.str.split(pat='-',expand=True).iloc[:,3].tolist()\n",
    "utt  = test_labels.utt.str.split(pat='-',expand=True).iloc[:,4].tolist()\n",
    "\n",
    "wavfile = list(zip(accent,speaker,utt))\n",
    "test_labels['wavfile'] = [wavdir+'/'.join(s)+'.wav' for s in wavfile]\n",
    "test_df = test_labels\n",
    "print(f'TEST set:\\n# utterances: {test_labels.shape[0]}\\n{test_labels.head()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1  Activate this to train on smaller subset of the dataset\n",
    "\n",
    "The subset is splitted into `train_on_subset['n_split']` with StratifiedShuffleSplit to maintain the proportion of the labels. One of the split is taken as training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12454\n",
      "Train Subset label distribution:\n",
      "0    1689\n",
      "1    1794\n",
      "2    1358\n",
      "3    1335\n",
      "4    1530\n",
      "5    1619\n",
      "6    1634\n",
      "7    1495\n",
      "dtype: int64\n",
      "1198\n",
      "Dev Subset label distribution:\n",
      "0    143\n",
      "1    158\n",
      "2    149\n",
      "3    131\n",
      "4    149\n",
      "5    146\n",
      "6    161\n",
      "7    161\n",
      "dtype: int64\n",
      "                                            utt  label  gender  age  \\\n",
      "0  AESRC2020-AMERICAN-ACCENT-G00473-G00473S1002      0       1   21   \n",
      "1  AESRC2020-AMERICAN-ACCENT-G00473-G00473S1004      0       1   21   \n",
      "2  AESRC2020-AMERICAN-ACCENT-G00473-G00473S1028      0       1   21   \n",
      "3  AESRC2020-AMERICAN-ACCENT-G00473-G00473S1040      0       1   21   \n",
      "4  AESRC2020-AMERICAN-ACCENT-G00473-G00473S1063      0       1   21   \n",
      "\n",
      "                           wavfile  \n",
      "0  audio/US/G00473/G00473S1002.wav  \n",
      "1  audio/US/G00473/G00473S1004.wav  \n",
      "2  audio/US/G00473/G00473S1028.wav  \n",
      "3  audio/US/G00473/G00473S1040.wav  \n",
      "4  audio/US/G00473/G00473S1063.wav  \n",
      "                                            utt  label  gender  age  \\\n",
      "0  AESRC2020-AMERICAN-ACCENT-G00007-G00007S1025      0       0   20   \n",
      "1  AESRC2020-AMERICAN-ACCENT-G00007-G00007S1073      0       0   20   \n",
      "2  AESRC2020-AMERICAN-ACCENT-G00007-G00007S1075      0       0   20   \n",
      "3  AESRC2020-AMERICAN-ACCENT-G00007-G00007S1100      0       0   20   \n",
      "4  AESRC2020-AMERICAN-ACCENT-G00007-G00007S1103      0       0   20   \n",
      "\n",
      "                           wavfile  \n",
      "0  audio/US/G00007/G00007S1025.wav  \n",
      "1  audio/US/G00007/G00007S1073.wav  \n",
      "2  audio/US/G00007/G00007S1075.wav  \n",
      "3  audio/US/G00007/G00007S1100.wav  \n",
      "4  audio/US/G00007/G00007S1103.wav  \n"
     ]
    }
   ],
   "source": [
    "if train_on_subset['activate'] == True:\n",
    "    #partial training set, smaller size\n",
    "    from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, train_size=train_on_subset['size'], random_state=100)\n",
    "    \n",
    "    #subset on training data\n",
    "    trlabels = train_labels.label.tolist()\n",
    "    trutt    = train_labels.wavfile.tolist()\n",
    "    \n",
    "    for tr, tst in sss.split(trutt, trlabels):\n",
    "        print(len(tr))\n",
    "        train_subset_idx = tr\n",
    "\n",
    "    train_subset = [trlabels[idx] for idx in train_subset_idx]\n",
    "    train_subset = pd.Series(train_subset)\n",
    "    print(f'Train Subset label distribution:\\n{train_subset.value_counts().sort_index()}')\n",
    "    \n",
    "    #subset on dev data\n",
    "    devlabels = dev_labels.label.tolist()\n",
    "    devutt    = dev_labels.wavfile.tolist()\n",
    "    \n",
    "    for tr, tst in sss.split(devutt, devlabels):\n",
    "        print(len(tr))\n",
    "        dev_subset_idx = tr\n",
    "\n",
    "    dev_subset = [devlabels[idx] for idx in dev_subset_idx]\n",
    "    dev_subset = pd.Series(dev_subset)\n",
    "    print(f'Dev Subset label distribution:\\n{dev_subset.value_counts().sort_index()}')\n",
    "    \n",
    "    #get subset dataframe\n",
    "    train_subset_labels = train_labels[train_labels.index.isin(list(train_subset_idx))].copy()\n",
    "    train_df = train_subset_labels.reset_index(drop=True)\n",
    "    print(train_df.head())\n",
    "    \n",
    "    dev_subset_labels = dev_labels[dev_labels.index.isin(list(dev_subset_idx))].copy()\n",
    "    dev_df = dev_subset_labels.reset_index(drop=True)\n",
    "    print(dev_df.head())\n",
    "    \n",
    "    feature_setting += '_partial'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Spectrogram Features\n",
    "\n",
    "Adapted from:\n",
    "https://github.com/samsudinng/VGGVox-PyTorch/blob/master/train.py\n",
    "\n",
    "The author converted the Matlab original implementation to Python (and verified). The pre-trained weights was trained on VoxCeleb1 dataset for speaker identification/verification.\n",
    "\n",
    "VGGVox features is magnitude spectrogram (512x300) with type `np.float32`. The frequency bins (512) is full mirrored spectrum (256+256) from 512-point FFT. Only the first 257 points are saved (spec\\[:257,:\\]) and the full features can be built by mirroring spec\\[1:256,:\\] and concatenating to the saved features.\n",
    "\n",
    "### 2.1 Audio to Spectrogram (VGGVox)\n",
    "   \n",
    "   - dc removal and dithering\n",
    "   - pre-emphasis filtering\n",
    "   - convert to spectrogram\n",
    "   - zscore standardization (per utterance)\n",
    "   - save all quantized features as dictionary (```all_spec```)\n",
    "      - key  : utterance name (eg. ```AESRC2020-AMERICAN-ACCENT-G00473-G00473S1028```)\n",
    "      - value: numpy array (`np.float32`), shape = (F, T) where F = 257 and should be mirrored to 512 as VGGVox feature\n",
    "   - write to .pkl.gz\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rm_dc_n_dither(audio):\n",
    "    # All files 16kHz tested..... Will copy for 8kHz from author's matlab code later\n",
    "    alpha=0.99\n",
    "    b=[1,-1]\n",
    "    a=[1,-alpha]\n",
    "\n",
    "    audio=lfilter(b,a,audio)\n",
    "\n",
    "    dither=np.random.uniform(low=-1,high=1, size=audio.shape)\n",
    "    spow=np.std(audio)\n",
    "    return audio+(1e-6*spow)*dither\n",
    "\n",
    "def preemphasis(audio, alpha=0.97):\n",
    "    b=[1, -alpha]\n",
    "    a=1\n",
    "    return lfilter(b, a, audio)\n",
    "\n",
    "def normalize_frames(m,epsilon=1e-12):\n",
    "    return (m-m.mean(1, keepdims=True))/np.clip(m.std(1, keepdims=True),epsilon, None)\n",
    "\n",
    "def preprocess(audio, buckets=None, sr=16000, Ws=25, Ss=10, alpha=0.97):\n",
    "    #ms to number of frames\n",
    "    if not buckets:\n",
    "        buckets={100: 2,\n",
    "             200: 5,\n",
    "             300: 8,\n",
    "             400: 11,\n",
    "             500: 14,\n",
    "             600: 17,\n",
    "             700: 20,\n",
    "             800: 23,\n",
    "             900: 27,\n",
    "             1000: 30}\n",
    "        \n",
    "    Nw=round((Ws*sr)/1000)\n",
    "    Ns=round((Ss*sr)/1000)\n",
    "\n",
    "\n",
    "    #hamming window func signature\n",
    "    window=np.hamming\n",
    "    #get next power of 2 greater than or equal to current Nw\n",
    "    nfft=1<<(Nw-1).bit_length()\n",
    "\n",
    "    # Remove DC and add small dither\n",
    "    audio=rm_dc_n_dither(audio)\n",
    "\n",
    "    # Preemphasis filtering\n",
    "    audio=preemphasis(audio, alpha)\n",
    "    \n",
    "     #get 512x300 spectrograms\n",
    "    _, _, mag=stft(audio,\n",
    "    fs=sr,\n",
    "    window=window(Nw),\n",
    "    nperseg=Nw,\n",
    "    noverlap=Nw-Ns,\n",
    "    nfft=nfft,\n",
    "    return_onesided=False,\n",
    "    padded=False,\n",
    "    boundary=None)\n",
    "\n",
    "    mag=normalize_frames(np.abs(mag))\n",
    "    #print(f\"ms {mag.shape}\")\n",
    "\n",
    "    #Get the largest bucket smaller than number of column vectors i.e. frames\n",
    "    #rsize=max(i for i in buckets if i<=mag.shape[1])\n",
    "    #rstart=(mag.shape[1]-rsize)//2\n",
    "    #Return truncated spectrograms\n",
    "    #return mag[:,rstart:rstart+rsize]\n",
    "    return mag[:257,:]\n",
    "\n",
    "def get_data_range(x):\n",
    "    \n",
    "    max_val = np.max(x.flatten())\n",
    "    min_val = np.min(x.flatten())\n",
    "    \n",
    "    return max_val, min_val\n",
    "\n",
    "def extract_feature(df, spec_params, data_range):\n",
    "    \n",
    "    all_spec={}\n",
    "    minmax_scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    \n",
    "    for row, col in tqdm(df.iterrows()):\n",
    "        utt = col['utt']\n",
    "        label = col['label']\n",
    "        wavfilename = col['wavfile']\n",
    "    \n",
    "        #Read utterance wav file\n",
    "        x, sr = librosa.load(wavfilename, sr=None)\n",
    "        assert sr == spec_params['sampling_freq']\n",
    "    \n",
    "        spec = preprocess(x, sr=sr, Ws=spec_params['win_len'], Ss = spec_params['hop_len'])\n",
    "        \n",
    "        vmax,vmin = get_data_range(spec)\n",
    "        data_range['max_ori'].append(vmax)\n",
    "        data_range['min_ori'].append(vmin)\n",
    "    \n",
    "        all_spec[utt] = np.flipud(spec).astype(np.float32)\n",
    "         \n",
    "    return all_spec, data_range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0/8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1556it [00:17, 87.26it/s]\n",
      "9it [00:00, 87.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1556it [00:17, 91.01it/s] \n",
      "6it [00:00, 59.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 2/8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1556it [00:20, 77.22it/s]\n",
      "7it [00:00, 62.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 3/8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1556it [00:21, 73.87it/s]\n"
     ]
    }
   ],
   "source": [
    "data_range = defaultdict(list)\n",
    "\n",
    "#split features into chunks\n",
    "nsplit = 8\n",
    "split_size = int(np.floor(len(train_df)/nsplit))\n",
    "split_start = [split_size*idx for idx in list(range(0,nsplit,1))]\n",
    "split_end = [split_size*idx for idx in list(range(1,nsplit,1))]\n",
    "split_end.append(len(train_df))\n",
    "split_idx = zip(split_start, split_end)\n",
    "\n",
    "\n",
    "for chunk, idx in enumerate(split_idx):\n",
    "    start,end = idx\n",
    "    \n",
    "    print(f'Chunk {chunk}/{nsplit}\\n')\n",
    "    all_spec, data_range = extract_feature(train_df.iloc[start:end,:], spec_params, data_range)\n",
    "\n",
    "    pickle.dump( all_spec, gzip.open( f'{outdir}train_{feature_setting}_{chunk}.pkl.gz',   'wb' ) )\n",
    "    \n",
    "    del all_spec\n",
    "    gc.collect()\n",
    "\n",
    "gmin = min(data_range['min_ori'])\n",
    "gmax = max(data_range['max_ori'])\n",
    "print(f'Data range original    : min: {gmin} - max: {gmax}')\n",
    "\n",
    "gmin = min(data_range['min_std'])\n",
    "gmax = max(data_range['max_std'])\n",
    "print(f'Data range standardized: min: {gmin} - max: {gmax}')\n",
    "\n",
    "del data_range\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump( all_spec, gzip.open( 'train_logspec200.pkl.gz',   'wb' ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(f'{outdir}train_{feature_setting}_0.pkl.gz', 'rb') as f:\n",
    "    allspec = pickle.load(f)\n",
    "\n",
    "utt = list(allspec.keys())[0]\n",
    "img = allspec[utt]\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(img[:,:500])\n",
    "plt.title(utt)\n",
    "plt.show()\n",
    "\n",
    "del allspec\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Dev data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_spec={}\n",
    "minmax_scaler = MinMaxScaler(feature_range=(0,1))\n",
    "data_range = defaultdict(list)\n",
    "\n",
    "all_spec, data_range = extract_feature(dev_df, spec_params, data_range)\n",
    "\n",
    "gmin = min(data_range['min_ori'])\n",
    "gmax = max(data_range['max_ori'])\n",
    "print(f'Data range original    : min: {gmin} - max: {gmax}')\n",
    "\n",
    "gmin = min(data_range['min_std'])\n",
    "gmax = max(data_range['max_std'])\n",
    "print(f'Data range standardized: min: {gmin} - max: {gmax}')\n",
    "pickle.dump( all_spec, gzip.open( f'{outdir}dev_{feature_setting}_0.pkl.gz',   'wb' ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(f'{outdir}dev_{feature_setting}_0.pkl.gz', 'rb') as f:\n",
    "    allspec = pickle.load(f)\n",
    "\n",
    "utt = list(allspec.keys())[0]\n",
    "img = allspec[utt]\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(img[:,:500])\n",
    "plt.title(utt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Test data (in-progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_spec={}\n",
    "minmax_scaler = MinMaxScaler(feature_range=(0,1))\n",
    "data_range = defaultdict(list)\n",
    "\n",
    "all_spec, data_range = extract_feature_image(test_df, spec_params, zscore_scaler, data_range)\n",
    "\n",
    "\n",
    "gmin = min(data_range['min_ori'])\n",
    "gmax = max(data_range['max_ori'])\n",
    "print(f'Data range original    : min: {gmin} - max: {gmax}')\n",
    "\n",
    "gmin = min(data_range['min_std'])\n",
    "gmax = max(data_range['max_std'])\n",
    "print(f'Data range standardized: min: {gmin} - max: {gmax}')\n",
    "pickle.dump( all_spec, gzip.open( f'{outdir}test_{feature_setting}_0.pkl.gz',   'wb' ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(f'{outdir}test_{feature_setting}_0.pkl.gz', 'rb') as f:\n",
    "    allspec = pickle.load(f)\n",
    "\n",
    "utt = list(allspec.keys())[0]\n",
    "img = allspec[utt]\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(img[:,:500])\n",
    "plt.title(utt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to try:\n",
    "\n",
    "#label smoothing\n",
    "#focal loss\n",
    "#multitask (gender)\n",
    "#language identification pre-trained model?\n",
    "#3-ch image: \n",
    "#    - spec, deltas\n",
    "#    - spec, different resolution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pykaldi",
   "language": "python",
   "name": "pykaldi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
